{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4399af63-e833-4aaf-bd8f-f05ac73dbdea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::701147072243:role/service-role/SageMaker-MLOpsNotebookRole\n",
      "sagemaker bucket: sagemaker-us-east-1-701147072243\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"huggingface_hub[cli]\" --upgrade --quiet\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cdbc0e-86e1-4dbb-b51f-04b9933b3169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b762d78-0c8b-4f6a-ad5b-b07354648dc8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7d4c32-5d42-4b69-bf87-facf762e303f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f03c4c-efcd-4ede-b111-c2baf7ecf4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9e1c56fa3b4cd9b94d93f52d1d578d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_15 (fa_cup INTEGER, player VARCHAR, fa_trophy VARCHAR)', 'role': 'system'}, {'content': 'What is the average FA Cup that has gary jones as the player, and an FA trophy greater than 5?', 'role': 'user'}, {'content': 'SELECT AVG(fa_cup) FROM table_name_15 WHERE player = \"gary jones\" AND fa_trophy > 5', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n",
    " \n",
    "print(dataset[\"train\"][345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39cad22-da75-42ce-873c-acaf6fddb284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:273: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1bbaea00914ee696d6a6d4b7d198cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224ab7f98bbd431d859d45ad36ea9f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-701147072243/datasets/text-to-sql/train_dataset.json\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-701147072243/?region=us-east-1&prefix=datasets/text-to-sql/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/text-to-sql'\n",
    " \n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    " \n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6def36-4bc8-496f-8d8d-083c92cf65bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/train_dataset.json', # path where sagemaker will save training dataset\n",
    "  'model_id': \"codellama/CodeLlama-7b-hf\",           # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 3,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch_fused\",                      # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'learning_rate': 2e-4,                             # learning rate, based on QLoRA paper\n",
    "  'bf16': True,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671ae637-2e71-4d14-b4e7-b3bbc957c016",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    " \n",
    "# define Training Job Name\n",
    "job_name = f'codellama-7b-hf-text-to-sql-exp1'\n",
    " \n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_sft.py',    # train script\n",
    "    source_dir           = './llm-sagemaker-sample/scripts/trl',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    environment          = {\n",
    "                            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "                            # \"HF_TOKEN\": \"REPALCE_WITH_YOUR_TOKEN\" # huggingface token to access gated models, e.g. llama 2\n",
    "                            },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e8415-adbb-488e-ba5b-34ad70e97b48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 05:13:50 Starting - Starting the training job...\n",
      "2024-05-01 05:13:50 Pending - Training job waiting for capacity...\n",
      "2024-05-01 05:14:44 Pending - Preparing the instances for training...\n",
      "2024-05-01 05:15:19 Downloading - Downloading input data...\n",
      "2024-05-01 05:15:34 Downloading - Downloading the training image..................\n",
      "2024-05-01 05:18:30 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,218 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,236 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,246 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,248 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:08,627 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.38.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.7/130.7 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.18.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.27.2 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bitsandbytes==0.42.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.42.0)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.7.11 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.8.2 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.5.6 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.5.6.tar.gz (2.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 111.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 5)) (1.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11->-r requirements.txt (line 6)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (13.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (1.6.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 125.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 57.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.0/280.0 kB 43.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 27.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl (183 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.4/183.4 kB 34.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.5.6-cp310-cp310-linux_x86_64.whl size=120352136 sha256=63ab5a3883b67719671e154beb91522e2901cbe4af0c5e031306a06a85df0be5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/a8/1c/88/b959d6818b98a46d61ba231683abb7523b89ac1a7ed1e0c206\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn, accelerate, transformers, datasets, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 2.3.6\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-2.3.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-2.3.6\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.15.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.27.2 datasets-2.18.0 flash-attn-2.5.6 peft-0.8.2 transformers-4.38.2 trl-0.7.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,169 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,170 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,209 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,237 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,266 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,278 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training/train_dataset.json\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_len\": 3072,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"codellama/CodeLlama-7b-hf\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"optim\": \"adamw_torch_fused\",\n",
      "        \"output_dir\": \"/tmp/tun\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"tensorboard\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_qlora\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-701147072243/codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_sft.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"codellama/CodeLlama-7b-hf\",\"num_train_epochs\":3,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_sft.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_sft\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-701147072243/codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"codellama/CodeLlama-7b-hf\",\"num_train_epochs\":3,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-701147072243/codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243/source/sourcedir.tar.gz\",\"module_name\":\"run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_sft.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training/train_dataset.json\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_len\",\"3072\",\"--merge_adapters\",\"True\",\"--model_id\",\"codellama/CodeLlama-7b-hf\",\"--num_train_epochs\",\"3\",\"--optim\",\"adamw_torch_fused\",\"--output_dir\",\"/tmp/tun\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"tensorboard\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_qlora\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training/train_dataset.json\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=3072\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=codellama/CodeLlama-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=adamw_torch_fused\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/tun\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=tensorboard\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_QLORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_sft.py --bf16 True --dataset_path /opt/ml/input/data/training/train_dataset.json --gradient_accumulation_steps 4 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_len 3072 --merge_adapters True --model_id codellama/CodeLlama-7b-hf --num_train_epochs 3 --optim adamw_torch_fused --output_dir /tmp/tun --per_device_train_batch_size 1 --report_to tensorboard --save_strategy epoch --tf32 True --use_qlora True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,279 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,279 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10000 examples [00:00, 670359.29 examples/s]\u001b[0m\n",
      "\u001b[34mUsing QLoRA\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/637 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|██████████| 637/637 [00:00<00:00, 6.98MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json: 100%|██████████| 25.1k/25.1k [00:00<00:00, 175MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:24, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:24, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:24, 406MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 168M/9.98G [00:00<00:25, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 210M/9.98G [00:00<00:24, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 262M/9.98G [00:00<00:23, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 315M/9.98G [00:00<00:22, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▎         | 367M/9.98G [00:00<00:22, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▍         | 419M/9.98G [00:00<00:21, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▍         | 472M/9.98G [00:01<00:21, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▌         | 524M/9.98G [00:01<00:20, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▌         | 577M/9.98G [00:01<00:20, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▋         | 629M/9.98G [00:01<00:19, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 682M/9.98G [00:01<00:20, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 734M/9.98G [00:01<00:19, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 786M/9.98G [00:01<00:19, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 839M/9.98G [00:01<00:19, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 891M/9.98G [00:02<00:19, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 944M/9.98G [00:02<00:19, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|▉         | 996M/9.98G [00:02<00:19, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.05G/9.98G [00:02<00:19, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.10G/9.98G [00:02<00:20, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.15G/9.98G [00:02<00:20, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.21G/9.98G [00:02<00:19, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.26G/9.98G [00:02<00:19, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.31G/9.98G [00:02<00:19, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:03<00:19, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▍        | 1.42G/9.98G [00:03<00:19, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▍        | 1.47G/9.98G [00:03<00:20, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▌        | 1.52G/9.98G [00:03<00:19, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:03<00:18, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▋        | 1.63G/9.98G [00:03<00:18, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.68G/9.98G [00:03<00:18, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.73G/9.98G [00:03<00:19, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:04<00:19, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.84G/9.98G [00:04<00:19, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.89G/9.98G [00:04<00:18, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.94G/9.98G [00:04<00:18, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:04<00:18, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|██        | 2.04G/9.98G [00:04<00:18, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:04<00:17, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.15G/9.98G [00:04<00:17, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.20G/9.98G [00:05<00:17, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.25G/9.98G [00:05<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.31G/9.98G [00:05<00:17, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▎       | 2.36G/9.98G [00:05<00:17, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.41G/9.98G [00:05<00:17, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.46G/9.98G [00:05<00:17, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:05<00:17, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.57G/9.98G [00:05<00:17, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▋       | 2.62G/9.98G [00:05<00:17, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.67G/9.98G [00:06<00:16, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:06<00:17, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:06<00:18, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.81G/9.98G [00:06<00:19, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▊       | 2.85G/9.98G [00:06<00:18, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.89G/9.98G [00:06<00:18, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.94G/9.98G [00:06<00:18, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|██▉       | 2.98G/9.98G [00:06<00:18, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|███       | 3.02G/9.98G [00:07<00:19, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.06G/9.98G [00:07<00:19, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:07<00:19, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.16G/9.98G [00:07<00:17, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.20G/9.98G [00:07<00:18, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.24G/9.98G [00:07<00:17, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.28G/9.98G [00:07<00:17, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.33G/9.98G [00:07<00:16, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.39G/9.98G [00:07<00:15, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.44G/9.98G [00:08<00:15, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:08<00:16, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▌      | 3.52G/9.98G [00:08<00:16, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▌      | 3.58G/9.98G [00:08<00:14, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▋      | 3.63G/9.98G [00:08<00:14, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.68G/9.98G [00:08<00:13, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.73G/9.98G [00:08<00:13, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.79G/9.98G [00:08<00:14, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:09<00:13, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.89G/9.98G [00:09<00:14, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.94G/9.98G [00:09<00:13, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|████      | 4.00G/9.98G [00:09<00:12, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.05G/9.98G [00:09<00:13, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.10G/9.98G [00:09<00:13, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:09<00:13, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.20G/9.98G [00:09<00:13, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.26G/9.98G [00:09<00:12, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.31G/9.98G [00:10<00:12, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▎     | 4.36G/9.98G [00:10<00:13, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.41G/9.98G [00:10<00:13, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▍     | 4.47G/9.98G [00:10<00:18, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▌     | 4.51G/9.98G [00:10<00:17, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:10<00:16, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.59G/9.98G [00:10<00:15, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▋     | 4.63G/9.98G [00:11<00:16, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.68G/9.98G [00:11<00:15, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.74G/9.98G [00:11<00:13, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.80G/9.98G [00:11<00:11, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:11<00:10, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.92G/9.98G [00:11<00:10, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|████▉     | 4.98G/9.98G [00:11<00:10, 493MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 5.03G/9.98G [00:11<00:10, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:12<00:09, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.15G/9.98G [00:12<00:09, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.21G/9.98G [00:12<00:09, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.27G/9.98G [00:12<00:08, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:12<00:09, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.39G/9.98G [00:12<00:09, 503MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▍    | 5.44G/9.98G [00:12<00:09, 488MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:13<00:14, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.54G/9.98G [00:13<00:15, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:13<00:13, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.64G/9.98G [00:13<00:12, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.69G/9.98G [00:13<00:11, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:13<00:10, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.81G/9.98G [00:13<00:09, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.86G/9.98G [00:13<00:08, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.92G/9.98G [00:13<00:08, 486MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|██████    | 5.99G/9.98G [00:14<00:07, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.04G/9.98G [00:14<00:08, 491MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.10G/9.98G [00:14<00:07, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.16G/9.98G [00:14<00:07, 502MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.22G/9.98G [00:14<00:07, 489MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:14<00:11, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.31G/9.98G [00:15<00:14, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▎   | 6.35G/9.98G [00:15<00:15, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.39G/9.98G [00:15<00:18, 199MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.42G/9.98G [00:15<00:17, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.45G/9.98G [00:15<00:17, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.48G/9.98G [00:16<00:17, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▌   | 6.51G/9.98G [00:16<00:17, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.54G/9.98G [00:16<00:16, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.57G/9.98G [00:16<00:16, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.61G/9.98G [00:16<00:16, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▋   | 6.63G/9.98G [00:16<00:16, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.65G/9.98G [00:16<00:16, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.68G/9.98G [00:17<00:15, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.71G/9.98G [00:17<00:15, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.74G/9.98G [00:17<00:15, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.77G/9.98G [00:17<00:15, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.79G/9.98G [00:17<00:15, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.83G/9.98G [00:17<00:15, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:17<00:14, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.89G/9.98G [00:18<00:14, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.92G/9.98G [00:18<00:14, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.95G/9.98G [00:18<00:14, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:18<00:14, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 6.99G/9.98G [00:18<00:14, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 7.03G/9.98G [00:18<00:14, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.06G/9.98G [00:18<00:13, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.09G/9.98G [00:19<00:13, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:19<00:13, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.14G/9.98G [00:19<00:13, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.16G/9.98G [00:19<00:13, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.18G/9.98G [00:19<00:13, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.21G/9.98G [00:19<00:13, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.24G/9.98G [00:19<00:13, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.27G/9.98G [00:19<00:12, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.30G/9.98G [00:20<00:12, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.33G/9.98G [00:20<00:12, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.36G/9.98G [00:20<00:12, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:20<00:12, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.40G/9.98G [00:20<00:12, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.42G/9.98G [00:20<00:13, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▍  | 7.46G/9.98G [00:20<00:11, 213MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.49G/9.98G [00:20<00:11, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.52G/9.98G [00:21<00:11, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.55G/9.98G [00:21<00:11, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.58G/9.98G [00:21<00:11, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▋  | 7.61G/9.98G [00:21<00:11, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.64G/9.98G [00:21<00:11, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.68G/9.98G [00:21<00:10, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.71G/9.98G [00:21<00:10, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.74G/9.98G [00:22<00:11, 200MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.76G/9.98G [00:22<00:11, 199MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.78G/9.98G [00:22<00:11, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.80G/9.98G [00:22<00:11, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.82G/9.98G [00:22<00:11, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▊  | 7.84G/9.98G [00:22<00:11, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.86G/9.98G [00:22<00:11, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.89G/9.98G [00:22<00:11, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.91G/9.98G [00:23<00:15, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|███████▉  | 7.96G/9.98G [00:23<00:09, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 7.99G/9.98G [00:23<00:10, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 8.02G/9.98G [00:23<00:10, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.05G/9.98G [00:23<00:10, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:23<00:10, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.10G/9.98G [00:24<00:09, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████▏ | 8.12G/9.98G [00:24<00:09, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.14G/9.98G [00:24<00:09, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.16G/9.98G [00:24<00:09, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.18G/9.98G [00:24<00:09, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.20G/9.98G [00:24<00:09, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.22G/9.98G [00:24<00:09, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.24G/9.98G [00:24<00:09, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.26G/9.98G [00:24<00:09, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.28G/9.98G [00:25<00:09, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.30G/9.98G [00:25<00:09, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.33G/9.98G [00:25<00:09, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▎ | 8.35G/9.98G [00:25<00:09, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.37G/9.98G [00:25<00:09, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.40G/9.98G [00:25<00:08, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:25<00:08, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.44G/9.98G [00:25<00:08, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.46G/9.98G [00:26<00:08, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.48G/9.98G [00:26<00:08, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.50G/9.98G [00:26<00:08, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [00:26<00:08, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.55G/9.98G [00:26<00:08, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.57G/9.98G [00:26<00:07, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.59G/9.98G [00:26<00:07, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▋ | 8.61G/9.98G [00:26<00:07, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▋ | 8.63G/9.98G [00:27<00:07, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.65G/9.98G [00:27<00:07, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [00:27<00:07, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.69G/9.98G [00:27<00:07, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.71G/9.98G [00:27<00:06, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [00:27<00:06, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.76G/9.98G [00:27<00:07, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [00:27<00:07, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.80G/9.98G [00:27<00:06, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.82G/9.98G [00:28<00:06, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [00:28<00:06, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [00:28<00:06, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.88G/9.98G [00:28<00:06, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:28<00:06, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.92G/9.98G [00:28<00:06, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.94G/9.98G [00:28<00:05, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.97G/9.98G [00:28<00:06, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 8.99G/9.98G [00:29<00:05, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 9.01G/9.98G [00:29<00:05, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 9.03G/9.98G [00:29<00:05, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:29<00:05, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.07G/9.98G [00:29<00:05, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.09G/9.98G [00:29<00:04, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████▏| 9.11G/9.98G [00:29<00:04, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.13G/9.98G [00:29<00:04, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [00:30<00:04, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.18G/9.98G [00:30<00:04, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.20G/9.98G [00:30<00:04, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.22G/9.98G [00:30<00:04, 167MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.24G/9.98G [00:30<00:04, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.26G/9.98G [00:30<00:04, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.28G/9.98G [00:30<00:04, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.30G/9.98G [00:31<00:04, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.32G/9.98G [00:31<00:04, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [00:31<00:04, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.36G/9.98G [00:31<00:03, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.38G/9.98G [00:31<00:03, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.41G/9.98G [00:31<00:03, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.43G/9.98G [00:31<00:03, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:31<00:03, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.47G/9.98G [00:32<00:03, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.49G/9.98G [00:32<00:03, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [00:32<00:02, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.53G/9.98G [00:32<00:02, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:32<00:02, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [00:32<00:02, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:32<00:02, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▋| 9.62G/9.98G [00:32<00:02, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:33<00:02, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.66G/9.98G [00:33<00:02, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.72G/9.98G [00:33<00:01, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.75G/9.98G [00:33<00:01, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [00:33<00:01, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.81G/9.98G [00:34<00:00, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▊| 9.84G/9.98G [00:34<00:00, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.86G/9.98G [00:34<00:00, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.88G/9.98G [00:34<00:00, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.90G/9.98G [00:34<00:00, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.92G/9.98G [00:34<00:00, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.94G/9.98G [00:34<00:00, 173MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.96G/9.98G [00:34<00:00, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:35<00:00, 284MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:35<00:35, 35.18s/it]\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   1%|          | 41.9M/3.50G [00:00<00:08, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   2%|▏         | 83.9M/3.50G [00:00<00:13, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   3%|▎         | 115M/3.50G [00:00<00:15, 214MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   4%|▍         | 147M/3.50G [00:00<00:17, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   5%|▍         | 168M/3.50G [00:00<00:17, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   5%|▌         | 189M/3.50G [00:00<00:17, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   6%|▌         | 210M/3.50G [00:01<00:18, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 231M/3.50G [00:01<00:17, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 262M/3.50G [00:01<00:15, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   8%|▊         | 294M/3.50G [00:01<00:14, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   9%|▉         | 325M/3.50G [00:01<00:13, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  10%|█         | 357M/3.50G [00:01<00:13, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  11%|█         | 388M/3.50G [00:01<00:12, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  12%|█▏        | 419M/3.50G [00:01<00:11, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  13%|█▎        | 451M/3.50G [00:01<00:11, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  14%|█▍        | 482M/3.50G [00:02<00:11, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  15%|█▍        | 514M/3.50G [00:02<00:11, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  16%|█▌        | 545M/3.50G [00:02<00:11, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  16%|█▋        | 577M/3.50G [00:02<00:11, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  17%|█▋        | 608M/3.50G [00:02<00:11, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  18%|█▊        | 640M/3.50G [00:02<00:11, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:02<00:10, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  20%|██        | 703M/3.50G [00:02<00:10, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  21%|██        | 734M/3.50G [00:03<00:10, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  22%|██▏       | 765M/3.50G [00:03<00:10, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  23%|██▎       | 797M/3.50G [00:03<00:10, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  24%|██▎       | 828M/3.50G [00:03<00:10, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▍       | 860M/3.50G [00:03<00:09, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▌       | 891M/3.50G [00:03<00:10, 256MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  26%|██▋       | 923M/3.50G [00:03<00:09, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  27%|██▋       | 954M/3.50G [00:03<00:09, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  28%|██▊       | 986M/3.50G [00:04<00:09, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  29%|██▉       | 1.02G/3.50G [00:04<00:09, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  30%|██▉       | 1.05G/3.50G [00:04<00:10, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  31%|███       | 1.08G/3.50G [00:04<00:12, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  31%|███▏      | 1.10G/3.50G [00:04<00:12, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  32%|███▏      | 1.12G/3.50G [00:04<00:13, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  33%|███▎      | 1.14G/3.50G [00:04<00:13, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  33%|███▎      | 1.16G/3.50G [00:05<00:13, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▍      | 1.18G/3.50G [00:05<00:13, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▍      | 1.21G/3.50G [00:05<00:13, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:05<00:13, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▌      | 1.25G/3.50G [00:05<00:13, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▌      | 1.27G/3.50G [00:05<00:13, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.29G/3.50G [00:05<00:13, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.31G/3.50G [00:05<00:13, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  38%|███▊      | 1.33G/3.50G [00:06<00:12, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▊      | 1.35G/3.50G [00:06<00:12, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▉      | 1.37G/3.50G [00:06<00:12, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  40%|███▉      | 1.39G/3.50G [00:06<00:12, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  40%|████      | 1.42G/3.50G [00:06<00:12, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  41%|████      | 1.44G/3.50G [00:06<00:12, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  42%|████▏     | 1.46G/3.50G [00:06<00:11, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  42%|████▏     | 1.48G/3.50G [00:06<00:12, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.50G/3.50G [00:07<00:11, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.52G/3.50G [00:07<00:12, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  44%|████▍     | 1.54G/3.50G [00:07<00:11, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  45%|████▍     | 1.56G/3.50G [00:07<00:11, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  45%|████▌     | 1.58G/3.50G [00:07<00:11, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▌     | 1.60G/3.50G [00:07<00:11, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▋     | 1.63G/3.50G [00:07<00:11, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  47%|████▋     | 1.65G/3.50G [00:07<00:11, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  48%|████▊     | 1.67G/3.50G [00:08<00:11, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  48%|████▊     | 1.69G/3.50G [00:08<00:15, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  50%|████▉     | 1.74G/3.50G [00:08<00:09, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  51%|█████     | 1.77G/3.50G [00:08<00:09, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 1.80G/3.50G [00:08<00:09, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 1.82G/3.50G [00:09<00:10, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 1.85G/3.50G [00:09<00:10, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 1.87G/3.50G [00:09<00:10, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:09<00:09, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:09<00:09, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▌    | 1.93G/3.50G [00:09<00:09, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▌    | 1.95G/3.50G [00:09<00:09, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▋    | 1.97G/3.50G [00:09<00:09, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  57%|█████▋    | 1.99G/3.50G [00:10<00:09, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.01G/3.50G [00:10<00:09, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.03G/3.50G [00:10<00:08, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▊    | 2.06G/3.50G [00:10<00:08, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▉    | 2.08G/3.50G [00:10<00:08, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:10<00:08, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████    | 2.12G/3.50G [00:10<00:08, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████    | 2.14G/3.50G [00:10<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.16G/3.50G [00:11<00:08, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.18G/3.50G [00:11<00:08, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  63%|██████▎   | 2.20G/3.50G [00:11<00:07, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:11<00:07, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▍   | 2.24G/3.50G [00:11<00:07, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  65%|██████▍   | 2.26G/3.50G [00:11<00:07, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  65%|██████▌   | 2.29G/3.50G [00:11<00:07, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  66%|██████▌   | 2.31G/3.50G [00:12<00:07, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:12<00:07, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 2.35G/3.50G [00:12<00:07, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 2.37G/3.50G [00:12<00:06, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 2.39G/3.50G [00:12<00:06, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:12<00:06, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▉   | 2.43G/3.50G [00:12<00:06, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  70%|███████   | 2.45G/3.50G [00:12<00:06, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████   | 2.47G/3.50G [00:13<00:06, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████▏  | 2.50G/3.50G [00:13<00:06, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  72%|███████▏  | 2.52G/3.50G [00:13<00:06, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:13<00:05, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  73%|███████▎  | 2.56G/3.50G [00:13<00:05, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▎  | 2.58G/3.50G [00:13<00:05, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▍  | 2.60G/3.50G [00:13<00:05, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:13<00:05, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▌  | 2.64G/3.50G [00:14<00:05, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  76%|███████▌  | 2.66G/3.50G [00:14<00:05, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 2.68G/3.50G [00:14<00:05, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 2.71G/3.50G [00:14<00:04, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:14<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 2.75G/3.50G [00:14<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  79%|███████▉  | 2.77G/3.50G [00:14<00:04, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:15<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|████████  | 2.81G/3.50G [00:15<00:04, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:15<00:04, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:15<00:04, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:15<00:03, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  83%|████████▎ | 2.92G/3.50G [00:15<00:03, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▍ | 2.94G/3.50G [00:15<00:03, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▍ | 2.96G/3.50G [00:16<00:03, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:16<00:03, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▌ | 3.00G/3.50G [00:16<00:03, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▋ | 3.02G/3.50G [00:16<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:16<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.06G/3.50G [00:16<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  88%|████████▊ | 3.08G/3.50G [00:16<00:02, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:16<00:02, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▉ | 3.12G/3.50G [00:17<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:17<00:02, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:17<00:02, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  91%|█████████ | 3.19G/3.50G [00:17<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  92%|█████████▏| 3.21G/3.50G [00:17<00:01, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  92%|█████████▏| 3.23G/3.50G [00:17<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 3.25G/3.50G [00:17<00:01, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:17<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  94%|█████████▍| 3.29G/3.50G [00:18<00:01, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  95%|█████████▍| 3.31G/3.50G [00:18<00:01, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  95%|█████████▌| 3.33G/3.50G [00:18<00:01, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  96%|█████████▌| 3.36G/3.50G [00:18<00:01, 117MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  97%|█████████▋| 3.41G/3.50G [00:18<00:00, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  98%|█████████▊| 3.44G/3.50G [00:18<00:00, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  99%|█████████▉| 3.47G/3.50G [00:19<00:00, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|█████████▉| 3.49G/3.50G [00:19<00:00, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:19<00:00, 180MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 25.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 27.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.42s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|██████████| 116/116 [00:00<00:00, 881kB/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 8.58MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.0MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 31.2MB/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 5.10MB/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mNo chat template is defined for this tokenizer - using the default template for the CodeLlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\u001b[0m\n",
      "\u001b[34mNo chat template is defined for this tokenizer - using the default template for the CodeLlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:01,  1.91s/ examples]\u001b[0m\n",
      "\u001b[34mGenerating train split: 247 examples [00:02, 170.19 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 416 examples [00:02, 193.46 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/312 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34m0%|          | 1/312 [00:18<1:36:52, 18.69s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/312 [00:36<1:35:04, 18.40s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/312 [00:55<1:34:17, 18.31s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 4/312 [01:13<1:33:45, 18.26s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 5/312 [01:31<1:33:19, 18.24s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/312 [01:49<1:32:57, 18.23s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/312 [02:07<1:32:36, 18.22s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 8/312 [02:26<1:32:16, 18.21s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/312 [02:44<1:31:56, 18.21s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/312 [03:02<1:31:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.943, 'grad_norm': 0.07861328125, 'learning_rate': 0.0002, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/312 [03:02<1:31:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 11/312 [03:20<1:31:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 12/312 [03:38<1:31:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 13/312 [03:57<1:30:41, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/312 [04:15<1:30:23, 18.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 15/312 [04:33<1:30:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 16/312 [04:51<1:29:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 17/312 [05:09<1:29:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 18/312 [05:28<1:29:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 19/312 [05:46<1:28:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 20/312 [06:04<1:28:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7615, 'grad_norm': 0.06884765625, 'learning_rate': 0.0002, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m6%|▋         | 20/312 [06:04<1:28:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 21/312 [06:22<1:28:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 22/312 [06:40<1:27:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/312 [06:59<1:27:39, 18.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 24/312 [07:17<1:27:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 25/312 [07:35<1:27:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 26/312 [07:53<1:26:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 27/312 [08:11<1:26:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 28/312 [08:30<1:26:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 29/312 [08:48<1:25:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 30/312 [09:06<1:25:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6877, 'grad_norm': 0.08251953125, 'learning_rate': 0.0002, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m10%|▉         | 30/312 [09:06<1:25:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 31/312 [09:24<1:25:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 32/312 [09:42<1:24:55, 18.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 33/312 [10:01<1:24:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 34/312 [10:19<1:24:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 35/312 [10:37<1:24:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 36/312 [10:55<1:23:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 37/312 [11:13<1:23:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 38/312 [11:32<1:23:11, 18.22s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 39/312 [11:50<1:22:52, 18.21s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 40/312 [12:08<1:22:32, 18.21s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.618, 'grad_norm': 0.1044921875, 'learning_rate': 0.0002, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 40/312 [12:08<1:22:32, 18.21s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 41/312 [12:26<1:22:13, 18.21s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 42/312 [12:44<1:21:55, 18.20s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 43/312 [13:03<1:21:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 44/312 [13:21<1:21:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 45/312 [13:39<1:20:59, 18.20s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 46/312 [13:57<1:20:41, 18.20s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 47/312 [14:15<1:20:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 48/312 [14:34<1:20:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 49/312 [14:52<1:19:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 50/312 [15:10<1:19:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5707, 'grad_norm': 0.0634765625, 'learning_rate': 0.0002, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 50/312 [15:10<1:19:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 51/312 [15:28<1:19:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 52/312 [15:46<1:18:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 53/312 [16:05<1:18:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 74/312 [22:27<1:12:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 75/312 [22:45<1:11:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 76/312 [23:03<1:11:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 77/312 [23:21<1:11:16, 18.20s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 78/312 [23:40<1:10:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 79/312 [23:58<1:10:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 80/312 [24:16<1:10:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5151, 'grad_norm': 0.0673828125, 'learning_rate': 0.0002, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 80/312 [24:16<1:10:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 81/312 [24:34<1:10:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 82/312 [24:52<1:09:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 83/312 [25:11<1:09:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 84/312 [25:29<1:09:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 85/312 [25:47<1:08:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 86/312 [26:05<1:08:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 87/312 [26:23<1:08:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 88/312 [26:42<1:07:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 89/312 [27:00<1:07:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 90/312 [27:18<1:07:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4943, 'grad_norm': 0.080078125, 'learning_rate': 0.0002, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 90/312 [27:18<1:07:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 91/312 [27:36<1:07:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 92/312 [27:54<1:06:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 93/312 [28:13<1:06:25, 18.20s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 94/312 [28:31<1:06:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 95/312 [28:49<1:05:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 96/312 [29:07<1:05:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 97/312 [29:25<1:05:12, 18.20s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 98/312 [29:44<1:04:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 99/312 [30:02<1:04:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 100/312 [30:20<1:04:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4998, 'grad_norm': 0.076171875, 'learning_rate': 0.0002, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 100/312 [30:20<1:04:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 101/312 [30:38<1:04:04, 18.22s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 102/312 [30:56<1:03:44, 18.21s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 103/312 [31:15<1:03:25, 18.21s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 104/312 [31:33<1:03:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 105/312 [31:51<1:03:17, 18.34s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 106/312 [32:10<1:02:49, 18.30s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 107/312 [32:28<1:02:25, 18.27s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 108/312 [32:46<1:02:02, 18.25s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 109/312 [33:04<1:01:41, 18.23s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 110/312 [33:22<1:01:21, 18.22s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4843, 'grad_norm': 0.07763671875, 'learning_rate': 0.0002, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 110/312 [33:22<1:01:21, 18.22s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 111/312 [33:41<1:01:01, 18.22s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 112/312 [33:59<1:00:41, 18.21s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 113/312 [34:17<1:00:23, 18.21s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 114/312 [34:35<1:00:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 115/312 [34:53<59:45, 18.20s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 116/312 [35:12<59:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 117/312 [35:30<59:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 118/312 [35:48<58:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 119/312 [36:06<58:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 120/312 [36:24<58:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.477, 'grad_norm': 0.08984375, 'learning_rate': 0.0002, 'epoch': 1.15}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 120/312 [36:24<58:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 121/312 [36:43<57:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 122/312 [37:01<57:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 123/312 [37:19<57:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 124/312 [37:37<57:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 125/312 [37:55<56:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 126/312 [38:14<56:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 127/312 [38:32<56:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 128/312 [38:50<55:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 129/312 [39:08<55:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 130/312 [39:26<55:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4697, 'grad_norm': 0.08544921875, 'learning_rate': 0.0002, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 130/312 [39:26<55:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 131/312 [39:45<54:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 132/312 [40:03<54:35, 18.20s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 133/312 [40:21<54:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 134/312 [40:39<53:59, 18.20s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 135/312 [40:57<53:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 136/312 [41:16<53:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 137/312 [41:34<53:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 138/312 [41:52<52:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 139/312 [42:10<52:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 140/312 [42:28<52:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4633, 'grad_norm': 0.07861328125, 'learning_rate': 0.0002, 'epoch': 1.35}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 140/312 [42:28<52:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 141/312 [42:47<51:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 142/312 [43:05<51:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 143/312 [43:23<51:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 144/312 [43:41<50:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 145/312 [43:59<50:39, 18.20s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 146/312 [44:18<50:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 147/312 [44:36<50:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 148/312 [44:54<49:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 149/312 [45:12<49:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 150/312 [45:30<49:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.465, 'grad_norm': 0.08349609375, 'learning_rate': 0.0002, 'epoch': 1.44}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 150/312 [45:30<49:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 151/312 [45:49<48:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 152/312 [46:07<48:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 153/312 [46:25<48:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 154/312 [46:43<47:58, 18.22s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 155/312 [47:01<47:39, 18.21s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 156/312 [47:20<47:20, 18.21s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 157/312 [47:38<47:01, 18.21s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 158/312 [47:56<46:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 159/312 [48:14<46:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 160/312 [48:32<46:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.454, 'grad_norm': 0.08740234375, 'learning_rate': 0.0002, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 160/312 [48:32<46:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 161/312 [48:51<45:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 162/312 [49:09<45:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 163/312 [49:27<45:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 164/312 [49:45<44:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 165/312 [50:03<44:35, 18.20s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 166/312 [50:22<44:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 167/312 [50:40<43:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 168/312 [50:58<43:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 169/312 [51:16<43:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 170/312 [51:34<43:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4539, 'grad_norm': 0.07861328125, 'learning_rate': 0.0002, 'epoch': 1.63}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 170/312 [51:34<43:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 171/312 [51:53<42:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 172/312 [52:11<42:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 173/312 [52:29<42:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 174/312 [52:47<41:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 175/312 [53:05<41:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 176/312 [53:24<41:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 177/312 [53:42<40:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 178/312 [54:00<40:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 179/312 [54:18<40:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 180/312 [54:36<40:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4509, 'grad_norm': 0.08056640625, 'learning_rate': 0.0002, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 180/312 [54:36<40:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 181/312 [54:55<39:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 182/312 [55:13<39:25, 18.20s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 183/312 [55:31<39:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 184/312 [55:49<38:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 185/312 [56:07<38:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 186/312 [56:26<38:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 187/312 [56:44<37:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 188/312 [57:02<37:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 189/312 [57:20<37:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 190/312 [57:38<37:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4461, 'grad_norm': 0.087890625, 'learning_rate': 0.0002, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[34m61%|██████    | 190/312 [57:38<37:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 191/312 [57:57<36:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 192/312 [58:15<36:23, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 193/312 [58:33<36:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 194/312 [58:51<35:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 195/312 [59:09<35:29, 18.20s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 196/312 [59:28<35:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 197/312 [59:46<34:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 198/312 [1:00:04<34:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 199/312 [1:00:22<34:16, 18.20s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 200/312 [1:00:40<33:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4558, 'grad_norm': 0.0869140625, 'learning_rate': 0.0002, 'epoch': 1.92}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 200/312 [1:00:40<33:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 201/312 [1:00:59<33:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 202/312 [1:01:17<33:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 203/312 [1:01:35<33:03, 18.20s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 204/312 [1:01:53<32:45, 18.20s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 205/312 [1:02:11<32:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 206/312 [1:02:30<32:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 207/312 [1:02:48<31:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 208/312 [1:03:06<31:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 209/312 [1:03:25<31:33, 18.39s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 210/312 [1:03:43<31:09, 18.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4383, 'grad_norm': 0.08251953125, 'learning_rate': 0.0002, 'epoch': 2.02}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 210/312 [1:03:43<31:09, 18.33s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 211/312 [1:04:01<30:49, 18.31s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 212/312 [1:04:19<30:27, 18.28s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 213/312 [1:04:38<30:07, 18.25s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 214/312 [1:04:56<29:47, 18.24s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 215/312 [1:05:14<29:27, 18.23s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 216/312 [1:05:32<29:08, 18.22s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 217/312 [1:05:50<28:50, 18.21s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 218/312 [1:06:09<28:31, 18.21s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 219/312 [1:06:27<28:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 220/312 [1:06:45<27:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4257, 'grad_norm': 0.08447265625, 'learning_rate': 0.0002, 'epoch': 2.12}\u001b[0m\n",
      "\u001b[34m71%|███████   | 220/312 [1:06:45<27:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 221/312 [1:07:03<27:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 222/312 [1:07:21<27:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 223/312 [1:07:40<27:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 224/312 [1:07:58<26:41, 18.20s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 225/312 [1:08:16<26:23, 18.20s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 226/312 [1:08:34<26:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 227/312 [1:08:52<25:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 228/312 [1:09:11<25:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 229/312 [1:09:29<25:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 230/312 [1:09:47<24:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4192, 'grad_norm': 0.0947265625, 'learning_rate': 0.0002, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 230/312 [1:09:47<24:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 231/312 [1:10:05<24:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 232/312 [1:10:23<24:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 233/312 [1:10:42<23:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 234/312 [1:11:00<23:39, 18.20s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 235/312 [1:11:18<23:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 236/312 [1:11:36<23:03, 18.20s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 237/312 [1:11:54<22:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 238/312 [1:12:13<22:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 239/312 [1:12:31<22:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 240/312 [1:12:49<21:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4149, 'grad_norm': 0.10205078125, 'learning_rate': 0.0002, 'epoch': 2.31}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 240/312 [1:12:49<21:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 241/312 [1:13:07<21:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 242/312 [1:13:25<21:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 243/312 [1:13:44<20:55, 18.20s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 244/312 [1:14:02<20:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 245/312 [1:14:20<20:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 246/312 [1:14:38<20:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 247/312 [1:14:56<19:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 248/312 [1:15:15<19:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 249/312 [1:15:33<19:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 250/312 [1:15:51<18:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4248, 'grad_norm': 0.0986328125, 'learning_rate': 0.0002, 'epoch': 2.4}\u001b[0m\n",
      "\u001b[34m80%|████████  | 250/312 [1:15:51<18:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 251/312 [1:16:09<18:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 252/312 [1:16:27<18:12, 18.20s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 253/312 [1:16:46<17:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 254/312 [1:17:04<17:35, 18.20s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 255/312 [1:17:22<17:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 256/312 [1:17:40<16:59, 18.20s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 257/312 [1:17:58<16:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 258/312 [1:18:17<16:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 259/312 [1:18:35<16:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 260/312 [1:18:53<15:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4198, 'grad_norm': 0.099609375, 'learning_rate': 0.0002, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 260/312 [1:18:53<15:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 261/312 [1:19:11<15:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 262/312 [1:19:29<15:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 263/312 [1:19:48<14:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 264/312 [1:20:06<14:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 265/312 [1:20:24<14:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 266/312 [1:20:42<13:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 267/312 [1:21:00<13:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 268/312 [1:21:19<13:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 269/312 [1:21:37<13:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 270/312 [1:21:55<12:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4163, 'grad_norm': 0.1005859375, 'learning_rate': 0.0002, 'epoch': 2.6}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 270/312 [1:21:55<12:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 271/312 [1:22:13<12:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 272/312 [1:22:31<12:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 273/312 [1:22:50<11:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 274/312 [1:23:08<11:32, 18.22s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 275/312 [1:23:26<11:13, 18.21s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 276/312 [1:23:44<10:55, 18.21s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 277/312 [1:24:02<10:37, 18.21s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 278/312 [1:24:21<10:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 279/312 [1:24:39<10:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 280/312 [1:24:57<09:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4197, 'grad_norm': 0.099609375, 'learning_rate': 0.0002, 'epoch': 2.69}\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 280/312 [1:24:57<09:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 281/312 [1:25:15<09:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 282/312 [1:25:33<09:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 283/312 [1:25:52<08:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 284/312 [1:26:10<08:29, 18.20s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 285/312 [1:26:28<08:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 286/312 [1:26:46<07:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 287/312 [1:27:04<07:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 288/312 [1:27:23<07:16, 18.20s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 289/312 [1:27:41<06:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 290/312 [1:27:59<06:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4145, 'grad_norm': 0.10498046875, 'learning_rate': 0.0002, 'epoch': 2.79}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 290/312 [1:27:59<06:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 291/312 [1:28:17<06:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 292/312 [1:28:35<06:03, 18.20s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 293/312 [1:28:54<05:45, 18.20s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 294/312 [1:29:12<05:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 295/312 [1:29:30<05:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 296/312 [1:29:48<04:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 297/312 [1:30:06<04:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 298/312 [1:30:25<04:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 299/312 [1:30:43<03:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 300/312 [1:31:01<03:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4177, 'grad_norm': 0.1123046875, 'learning_rate': 0.0002, 'epoch': 2.88}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 300/312 [1:31:01<03:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 301/312 [1:31:19<03:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 302/312 [1:31:37<03:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 303/312 [1:31:56<02:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 304/312 [1:32:14<02:25, 18.20s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 305/312 [1:32:32<02:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 306/312 [1:32:50<01:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 307/312 [1:33:08<01:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 308/312 [1:33:27<01:12, 18.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 309/312 [1:33:45<00:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 310/312 [1:34:03<00:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4224, 'grad_norm': 0.1015625, 'learning_rate': 0.0002, 'epoch': 2.98}\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 310/312 [1:34:03<00:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 311/312 [1:34:21<00:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 312/312 [1:34:39<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 5680.3818, 'train_samples_per_second': 0.22, 'train_steps_per_second': 0.055, 'train_loss': 0.49664624474751645, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 312/312 [1:34:40<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 312/312 [1:34:40<00:00, 18.21s/it]\u001b[0m\n",
      "\u001b[34m['README.md', 'adapter_config.json', 'runs', 'checkpoint-208', 'checkpoint-312', 'tokenizer.json', 'adapter_model.safetensors', 'special_tokens_map.json', 'tokenizer.model', 'checkpoint-104', 'tokenizer_config.json']\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 14.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.32s/it]\u001b[0m\n",
      "\u001b[34m2024-05-01 06:57:16,301 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-01 06:57:16,301 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-01 06:57:16,301 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-05-01 06:57:20 Uploading - Uploading generated training model\n",
      "2024-05-01 06:58:06 Completed - Training job completed\n",
      "Training seconds: 6167\n",
      "Billable seconds: 6167\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f6334-1929-4980-919d-cffc25222fe8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 05:13:50 Starting - Starting the training job...\n",
      "2024-05-01 05:13:50 Pending - Training job waiting for capacity...\n",
      "2024-05-01 05:14:44 Pending - Preparing the instances for training...\n",
      "2024-05-01 05:15:19 Downloading - Downloading input data...\n",
      "2024-05-01 05:15:34 Downloading - Downloading the training image..................\n",
      "2024-05-01 05:18:30 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,218 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,236 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,246 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:07,248 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:08,627 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.38.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.7/130.7 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.18.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.27.2 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bitsandbytes==0.42.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.42.0)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.7.11 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.8.2 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.5.6 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.5.6.tar.gz (2.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 111.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2->-r requirements.txt (line 1)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 5)) (1.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11->-r requirements.txt (line 6)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.6->-r requirements.txt (line 8)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (13.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (1.6.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 125.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 57.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.0/280.0 kB 43.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.7.11-py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 27.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.8.2-py3-none-any.whl (183 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.4/183.4 kB 34.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.5.6-cp310-cp310-linux_x86_64.whl size=120352136 sha256=63ab5a3883b67719671e154beb91522e2901cbe4af0c5e031306a06a85df0be5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/a8/1c/88/b959d6818b98a46d61ba231683abb7523b89ac1a7ed1e0c206\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn, accelerate, transformers, datasets, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 2.3.6\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-2.3.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-2.3.6\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.15.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.27.2 datasets-2.18.0 flash-attn-2.5.6 peft-0.8.2 transformers-4.38.2 trl-0.7.11\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,169 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,170 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,209 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,237 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,266 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,278 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training/train_dataset.json\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_len\": 3072,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"codellama/CodeLlama-7b-hf\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"optim\": \"adamw_torch_fused\",\n",
      "        \"output_dir\": \"/tmp/tun\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"tensorboard\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_qlora\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-701147072243/codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_sft.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"codellama/CodeLlama-7b-hf\",\"num_train_epochs\":3,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_sft.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_sft\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-701147072243/codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":3072,\"merge_adapters\":true,\"model_id\":\"codellama/CodeLlama-7b-hf\",\"num_train_epochs\":3,\"optim\":\"adamw_torch_fused\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-701147072243/codellama-7b-hf-text-to-sql-exp1-2024-05-01-05-13-50-243/source/sourcedir.tar.gz\",\"module_name\":\"run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_sft.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training/train_dataset.json\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_len\",\"3072\",\"--merge_adapters\",\"True\",\"--model_id\",\"codellama/CodeLlama-7b-hf\",\"--num_train_epochs\",\"3\",\"--optim\",\"adamw_torch_fused\",\"--output_dir\",\"/tmp/tun\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"tensorboard\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_qlora\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training/train_dataset.json\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=3072\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=codellama/CodeLlama-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=adamw_torch_fused\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/tun\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=tensorboard\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_QLORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_sft.py --bf16 True --dataset_path /opt/ml/input/data/training/train_dataset.json --gradient_accumulation_steps 4 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_len 3072 --merge_adapters True --model_id codellama/CodeLlama-7b-hf --num_train_epochs 3 --optim adamw_torch_fused --output_dir /tmp/tun --per_device_train_batch_size 1 --report_to tensorboard --save_strategy epoch --tf32 True --use_qlora True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,279 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-05-01 05:19:27,279 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10000 examples [00:00, 670359.29 examples/s]\u001b[0m\n",
      "\u001b[34mUsing QLoRA\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/637 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|██████████| 637/637 [00:00<00:00, 6.98MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json: 100%|██████████| 25.1k/25.1k [00:00<00:00, 175MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:24, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:24, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:24, 406MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 168M/9.98G [00:00<00:25, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 210M/9.98G [00:00<00:24, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 262M/9.98G [00:00<00:23, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 315M/9.98G [00:00<00:22, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▎         | 367M/9.98G [00:00<00:22, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▍         | 419M/9.98G [00:00<00:21, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▍         | 472M/9.98G [00:01<00:21, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▌         | 524M/9.98G [00:01<00:20, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▌         | 577M/9.98G [00:01<00:20, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▋         | 629M/9.98G [00:01<00:19, 469MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 682M/9.98G [00:01<00:20, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 734M/9.98G [00:01<00:19, 471MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 786M/9.98G [00:01<00:19, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 839M/9.98G [00:01<00:19, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 891M/9.98G [00:02<00:19, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 944M/9.98G [00:02<00:19, 472MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|▉         | 996M/9.98G [00:02<00:19, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.05G/9.98G [00:02<00:19, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.10G/9.98G [00:02<00:20, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.15G/9.98G [00:02<00:20, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.21G/9.98G [00:02<00:19, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.26G/9.98G [00:02<00:19, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.31G/9.98G [00:02<00:19, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:03<00:19, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▍        | 1.42G/9.98G [00:03<00:19, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▍        | 1.47G/9.98G [00:03<00:20, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▌        | 1.52G/9.98G [00:03<00:19, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:03<00:18, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▋        | 1.63G/9.98G [00:03<00:18, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.68G/9.98G [00:03<00:18, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.73G/9.98G [00:03<00:19, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:04<00:19, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.84G/9.98G [00:04<00:19, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.89G/9.98G [00:04<00:18, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.94G/9.98G [00:04<00:18, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:04<00:18, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|██        | 2.04G/9.98G [00:04<00:18, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██        | 2.10G/9.98G [00:04<00:17, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.15G/9.98G [00:04<00:17, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.20G/9.98G [00:05<00:17, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.25G/9.98G [00:05<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.31G/9.98G [00:05<00:17, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▎       | 2.36G/9.98G [00:05<00:17, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.41G/9.98G [00:05<00:17, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.46G/9.98G [00:05<00:17, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:05<00:17, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.57G/9.98G [00:05<00:17, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▋       | 2.62G/9.98G [00:05<00:17, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.67G/9.98G [00:06<00:16, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:06<00:17, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:06<00:18, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.81G/9.98G [00:06<00:19, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▊       | 2.85G/9.98G [00:06<00:18, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.89G/9.98G [00:06<00:18, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.94G/9.98G [00:06<00:18, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|██▉       | 2.98G/9.98G [00:06<00:18, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|███       | 3.02G/9.98G [00:07<00:19, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.06G/9.98G [00:07<00:19, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:07<00:19, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.16G/9.98G [00:07<00:17, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.20G/9.98G [00:07<00:18, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.24G/9.98G [00:07<00:17, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.28G/9.98G [00:07<00:17, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.33G/9.98G [00:07<00:16, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.39G/9.98G [00:07<00:15, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.44G/9.98G [00:08<00:15, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:08<00:16, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▌      | 3.52G/9.98G [00:08<00:16, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▌      | 3.58G/9.98G [00:08<00:14, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▋      | 3.63G/9.98G [00:08<00:14, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.68G/9.98G [00:08<00:13, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.73G/9.98G [00:08<00:13, 465MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.79G/9.98G [00:08<00:14, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:09<00:13, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.89G/9.98G [00:09<00:14, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.94G/9.98G [00:09<00:13, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|████      | 4.00G/9.98G [00:09<00:12, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.05G/9.98G [00:09<00:13, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.10G/9.98G [00:09<00:13, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:09<00:13, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.20G/9.98G [00:09<00:13, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.26G/9.98G [00:09<00:12, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.31G/9.98G [00:10<00:12, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▎     | 4.36G/9.98G [00:10<00:13, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.41G/9.98G [00:10<00:13, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▍     | 4.47G/9.98G [00:10<00:18, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▌     | 4.51G/9.98G [00:10<00:17, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:10<00:16, 333MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.59G/9.98G [00:10<00:15, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▋     | 4.63G/9.98G [00:11<00:16, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.68G/9.98G [00:11<00:15, 344MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.74G/9.98G [00:11<00:13, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.80G/9.98G [00:11<00:11, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:11<00:10, 470MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.92G/9.98G [00:11<00:10, 473MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|████▉     | 4.98G/9.98G [00:11<00:10, 493MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 5.03G/9.98G [00:11<00:10, 485MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:12<00:09, 492MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.15G/9.98G [00:12<00:09, 514MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.21G/9.98G [00:12<00:09, 520MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.27G/9.98G [00:12<00:08, 529MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:12<00:09, 513MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.39G/9.98G [00:12<00:09, 503MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▍    | 5.44G/9.98G [00:12<00:09, 488MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:13<00:14, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.54G/9.98G [00:13<00:15, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:13<00:13, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.64G/9.98G [00:13<00:12, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.69G/9.98G [00:13<00:11, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:13<00:10, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.81G/9.98G [00:13<00:09, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.86G/9.98G [00:13<00:08, 463MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.92G/9.98G [00:13<00:08, 486MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|██████    | 5.99G/9.98G [00:14<00:07, 500MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.04G/9.98G [00:14<00:08, 491MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.10G/9.98G [00:14<00:07, 505MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.16G/9.98G [00:14<00:07, 502MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.22G/9.98G [00:14<00:07, 489MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:14<00:11, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.31G/9.98G [00:15<00:14, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▎   | 6.35G/9.98G [00:15<00:15, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.39G/9.98G [00:15<00:18, 199MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.42G/9.98G [00:15<00:17, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.45G/9.98G [00:15<00:17, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.48G/9.98G [00:16<00:17, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▌   | 6.51G/9.98G [00:16<00:17, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.54G/9.98G [00:16<00:16, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.57G/9.98G [00:16<00:16, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.61G/9.98G [00:16<00:16, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▋   | 6.63G/9.98G [00:16<00:16, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.65G/9.98G [00:16<00:16, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.68G/9.98G [00:17<00:15, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.71G/9.98G [00:17<00:15, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.74G/9.98G [00:17<00:15, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.77G/9.98G [00:17<00:15, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.79G/9.98G [00:17<00:15, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.83G/9.98G [00:17<00:15, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:17<00:14, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.89G/9.98G [00:18<00:14, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.92G/9.98G [00:18<00:14, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.95G/9.98G [00:18<00:14, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:18<00:14, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 6.99G/9.98G [00:18<00:14, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 7.03G/9.98G [00:18<00:14, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.06G/9.98G [00:18<00:13, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.09G/9.98G [00:19<00:13, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:19<00:13, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.14G/9.98G [00:19<00:13, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.16G/9.98G [00:19<00:13, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.18G/9.98G [00:19<00:13, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.21G/9.98G [00:19<00:13, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.24G/9.98G [00:19<00:13, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.27G/9.98G [00:19<00:12, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.30G/9.98G [00:20<00:12, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.33G/9.98G [00:20<00:12, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.36G/9.98G [00:20<00:12, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:20<00:12, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.40G/9.98G [00:20<00:12, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.42G/9.98G [00:20<00:13, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▍  | 7.46G/9.98G [00:20<00:11, 213MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.49G/9.98G [00:20<00:11, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.52G/9.98G [00:21<00:11, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.55G/9.98G [00:21<00:11, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.58G/9.98G [00:21<00:11, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▋  | 7.61G/9.98G [00:21<00:11, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.64G/9.98G [00:21<00:11, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.68G/9.98G [00:21<00:10, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.71G/9.98G [00:21<00:10, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.74G/9.98G [00:22<00:11, 200MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.76G/9.98G [00:22<00:11, 199MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.78G/9.98G [00:22<00:11, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.80G/9.98G [00:22<00:11, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.82G/9.98G [00:22<00:11, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▊  | 7.84G/9.98G [00:22<00:11, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.86G/9.98G [00:22<00:11, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.89G/9.98G [00:22<00:11, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.91G/9.98G [00:23<00:15, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|███████▉  | 7.96G/9.98G [00:23<00:09, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 7.99G/9.98G [00:23<00:10, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 8.02G/9.98G [00:23<00:10, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.05G/9.98G [00:23<00:10, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:23<00:10, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.10G/9.98G [00:24<00:09, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████▏ | 8.12G/9.98G [00:24<00:09, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.14G/9.98G [00:24<00:09, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.16G/9.98G [00:24<00:09, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.18G/9.98G [00:24<00:09, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.20G/9.98G [00:24<00:09, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.22G/9.98G [00:24<00:09, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.24G/9.98G [00:24<00:09, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.26G/9.98G [00:24<00:09, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.28G/9.98G [00:25<00:09, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.30G/9.98G [00:25<00:09, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.33G/9.98G [00:25<00:09, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▎ | 8.35G/9.98G [00:25<00:09, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.37G/9.98G [00:25<00:09, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.40G/9.98G [00:25<00:08, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:25<00:08, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.44G/9.98G [00:25<00:08, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.46G/9.98G [00:26<00:08, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.48G/9.98G [00:26<00:08, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.50G/9.98G [00:26<00:08, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [00:26<00:08, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.55G/9.98G [00:26<00:08, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.57G/9.98G [00:26<00:07, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.59G/9.98G [00:26<00:07, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▋ | 8.61G/9.98G [00:26<00:07, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▋ | 8.63G/9.98G [00:27<00:07, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.65G/9.98G [00:27<00:07, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [00:27<00:07, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.69G/9.98G [00:27<00:07, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.71G/9.98G [00:27<00:06, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [00:27<00:06, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.76G/9.98G [00:27<00:07, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [00:27<00:07, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.80G/9.98G [00:27<00:06, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.82G/9.98G [00:28<00:06, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [00:28<00:06, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [00:28<00:06, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.88G/9.98G [00:28<00:06, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:28<00:06, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.92G/9.98G [00:28<00:06, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.94G/9.98G [00:28<00:05, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.97G/9.98G [00:28<00:06, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 8.99G/9.98G [00:29<00:05, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 9.01G/9.98G [00:29<00:05, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 9.03G/9.98G [00:29<00:05, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:29<00:05, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.07G/9.98G [00:29<00:05, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.09G/9.98G [00:29<00:04, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████▏| 9.11G/9.98G [00:29<00:04, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.13G/9.98G [00:29<00:04, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [00:30<00:04, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.18G/9.98G [00:30<00:04, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.20G/9.98G [00:30<00:04, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.22G/9.98G [00:30<00:04, 167MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.24G/9.98G [00:30<00:04, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.26G/9.98G [00:30<00:04, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.28G/9.98G [00:30<00:04, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.30G/9.98G [00:31<00:04, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.32G/9.98G [00:31<00:04, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [00:31<00:04, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.36G/9.98G [00:31<00:03, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.38G/9.98G [00:31<00:03, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.41G/9.98G [00:31<00:03, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.43G/9.98G [00:31<00:03, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:31<00:03, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.47G/9.98G [00:32<00:03, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.49G/9.98G [00:32<00:03, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [00:32<00:02, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.53G/9.98G [00:32<00:02, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:32<00:02, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [00:32<00:02, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:32<00:02, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▋| 9.62G/9.98G [00:32<00:02, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:33<00:02, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.66G/9.98G [00:33<00:02, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.72G/9.98G [00:33<00:01, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.75G/9.98G [00:33<00:01, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [00:33<00:01, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.81G/9.98G [00:34<00:00, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▊| 9.84G/9.98G [00:34<00:00, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.86G/9.98G [00:34<00:00, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.88G/9.98G [00:34<00:00, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.90G/9.98G [00:34<00:00, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.92G/9.98G [00:34<00:00, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.94G/9.98G [00:34<00:00, 173MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.96G/9.98G [00:34<00:00, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:35<00:00, 284MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:35<00:35, 35.18s/it]\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   1%|          | 41.9M/3.50G [00:00<00:08, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   2%|▏         | 83.9M/3.50G [00:00<00:13, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   3%|▎         | 115M/3.50G [00:00<00:15, 214MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   4%|▍         | 147M/3.50G [00:00<00:17, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   5%|▍         | 168M/3.50G [00:00<00:17, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   5%|▌         | 189M/3.50G [00:00<00:17, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   6%|▌         | 210M/3.50G [00:01<00:18, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 231M/3.50G [00:01<00:17, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 262M/3.50G [00:01<00:15, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   8%|▊         | 294M/3.50G [00:01<00:14, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   9%|▉         | 325M/3.50G [00:01<00:13, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  10%|█         | 357M/3.50G [00:01<00:13, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  11%|█         | 388M/3.50G [00:01<00:12, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  12%|█▏        | 419M/3.50G [00:01<00:11, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  13%|█▎        | 451M/3.50G [00:01<00:11, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  14%|█▍        | 482M/3.50G [00:02<00:11, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  15%|█▍        | 514M/3.50G [00:02<00:11, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  16%|█▌        | 545M/3.50G [00:02<00:11, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  16%|█▋        | 577M/3.50G [00:02<00:11, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  17%|█▋        | 608M/3.50G [00:02<00:11, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  18%|█▊        | 640M/3.50G [00:02<00:11, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:02<00:10, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  20%|██        | 703M/3.50G [00:02<00:10, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  21%|██        | 734M/3.50G [00:03<00:10, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  22%|██▏       | 765M/3.50G [00:03<00:10, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  23%|██▎       | 797M/3.50G [00:03<00:10, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  24%|██▎       | 828M/3.50G [00:03<00:10, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▍       | 860M/3.50G [00:03<00:09, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▌       | 891M/3.50G [00:03<00:10, 256MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  26%|██▋       | 923M/3.50G [00:03<00:09, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  27%|██▋       | 954M/3.50G [00:03<00:09, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  28%|██▊       | 986M/3.50G [00:04<00:09, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  29%|██▉       | 1.02G/3.50G [00:04<00:09, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  30%|██▉       | 1.05G/3.50G [00:04<00:10, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  31%|███       | 1.08G/3.50G [00:04<00:12, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  31%|███▏      | 1.10G/3.50G [00:04<00:12, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  32%|███▏      | 1.12G/3.50G [00:04<00:13, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  33%|███▎      | 1.14G/3.50G [00:04<00:13, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  33%|███▎      | 1.16G/3.50G [00:05<00:13, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▍      | 1.18G/3.50G [00:05<00:13, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▍      | 1.21G/3.50G [00:05<00:13, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:05<00:13, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▌      | 1.25G/3.50G [00:05<00:13, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▌      | 1.27G/3.50G [00:05<00:13, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.29G/3.50G [00:05<00:13, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.31G/3.50G [00:05<00:13, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  38%|███▊      | 1.33G/3.50G [00:06<00:12, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▊      | 1.35G/3.50G [00:06<00:12, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▉      | 1.37G/3.50G [00:06<00:12, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  40%|███▉      | 1.39G/3.50G [00:06<00:12, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  40%|████      | 1.42G/3.50G [00:06<00:12, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  41%|████      | 1.44G/3.50G [00:06<00:12, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  42%|████▏     | 1.46G/3.50G [00:06<00:11, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  42%|████▏     | 1.48G/3.50G [00:06<00:12, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.50G/3.50G [00:07<00:11, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.52G/3.50G [00:07<00:12, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  44%|████▍     | 1.54G/3.50G [00:07<00:11, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  45%|████▍     | 1.56G/3.50G [00:07<00:11, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  45%|████▌     | 1.58G/3.50G [00:07<00:11, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▌     | 1.60G/3.50G [00:07<00:11, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▋     | 1.63G/3.50G [00:07<00:11, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  47%|████▋     | 1.65G/3.50G [00:07<00:11, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  48%|████▊     | 1.67G/3.50G [00:08<00:11, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  48%|████▊     | 1.69G/3.50G [00:08<00:15, 114MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  50%|████▉     | 1.74G/3.50G [00:08<00:09, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  51%|█████     | 1.77G/3.50G [00:08<00:09, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 1.80G/3.50G [00:08<00:09, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 1.82G/3.50G [00:09<00:10, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 1.85G/3.50G [00:09<00:10, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 1.87G/3.50G [00:09<00:10, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:09<00:09, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:09<00:09, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▌    | 1.93G/3.50G [00:09<00:09, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▌    | 1.95G/3.50G [00:09<00:09, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▋    | 1.97G/3.50G [00:09<00:09, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  57%|█████▋    | 1.99G/3.50G [00:10<00:09, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.01G/3.50G [00:10<00:09, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.03G/3.50G [00:10<00:08, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▊    | 2.06G/3.50G [00:10<00:08, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▉    | 2.08G/3.50G [00:10<00:08, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:10<00:08, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████    | 2.12G/3.50G [00:10<00:08, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████    | 2.14G/3.50G [00:10<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.16G/3.50G [00:11<00:08, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.18G/3.50G [00:11<00:08, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  63%|██████▎   | 2.20G/3.50G [00:11<00:07, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:11<00:07, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▍   | 2.24G/3.50G [00:11<00:07, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  65%|██████▍   | 2.26G/3.50G [00:11<00:07, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  65%|██████▌   | 2.29G/3.50G [00:11<00:07, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  66%|██████▌   | 2.31G/3.50G [00:12<00:07, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:12<00:07, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 2.35G/3.50G [00:12<00:07, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 2.37G/3.50G [00:12<00:06, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 2.39G/3.50G [00:12<00:06, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:12<00:06, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▉   | 2.43G/3.50G [00:12<00:06, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  70%|███████   | 2.45G/3.50G [00:12<00:06, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████   | 2.47G/3.50G [00:13<00:06, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████▏  | 2.50G/3.50G [00:13<00:06, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  72%|███████▏  | 2.52G/3.50G [00:13<00:06, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:13<00:05, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  73%|███████▎  | 2.56G/3.50G [00:13<00:05, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▎  | 2.58G/3.50G [00:13<00:05, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▍  | 2.60G/3.50G [00:13<00:05, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:13<00:05, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▌  | 2.64G/3.50G [00:14<00:05, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  76%|███████▌  | 2.66G/3.50G [00:14<00:05, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 2.68G/3.50G [00:14<00:05, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 2.71G/3.50G [00:14<00:04, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:14<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 2.75G/3.50G [00:14<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  79%|███████▉  | 2.77G/3.50G [00:14<00:04, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:15<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|████████  | 2.81G/3.50G [00:15<00:04, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:15<00:04, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:15<00:04, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:15<00:03, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  83%|████████▎ | 2.92G/3.50G [00:15<00:03, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▍ | 2.94G/3.50G [00:15<00:03, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▍ | 2.96G/3.50G [00:16<00:03, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:16<00:03, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▌ | 3.00G/3.50G [00:16<00:03, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▋ | 3.02G/3.50G [00:16<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:16<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.06G/3.50G [00:16<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  88%|████████▊ | 3.08G/3.50G [00:16<00:02, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:16<00:02, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▉ | 3.12G/3.50G [00:17<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:17<00:02, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:17<00:02, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  91%|█████████ | 3.19G/3.50G [00:17<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  92%|█████████▏| 3.21G/3.50G [00:17<00:01, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  92%|█████████▏| 3.23G/3.50G [00:17<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 3.25G/3.50G [00:17<00:01, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:17<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  94%|█████████▍| 3.29G/3.50G [00:18<00:01, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  95%|█████████▍| 3.31G/3.50G [00:18<00:01, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  95%|█████████▌| 3.33G/3.50G [00:18<00:01, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  96%|█████████▌| 3.36G/3.50G [00:18<00:01, 117MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  97%|█████████▋| 3.41G/3.50G [00:18<00:00, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  98%|█████████▊| 3.44G/3.50G [00:18<00:00, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  99%|█████████▉| 3.47G/3.50G [00:19<00:00, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|█████████▉| 3.49G/3.50G [00:19<00:00, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:19<00:00, 180MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 25.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:54<00:00, 27.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.42s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|██████████| 116/116 [00:00<00:00, 881kB/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 8.58MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.0MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 31.2MB/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 5.10MB/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mNo chat template is defined for this tokenizer - using the default template for the CodeLlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\u001b[0m\n",
      "\u001b[34mNo chat template is defined for this tokenizer - using the default template for the CodeLlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:01,  1.91s/ examples]\u001b[0m\n",
      "\u001b[34mGenerating train split: 247 examples [00:02, 170.19 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 416 examples [00:02, 193.46 examples/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/312 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\u001b[0m\n",
      "\u001b[34m0%|          | 1/312 [00:18<1:36:52, 18.69s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/312 [00:36<1:35:04, 18.40s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/312 [00:55<1:34:17, 18.31s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 4/312 [01:13<1:33:45, 18.26s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 5/312 [01:31<1:33:19, 18.24s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/312 [01:49<1:32:57, 18.23s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/312 [02:07<1:32:36, 18.22s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 8/312 [02:26<1:32:16, 18.21s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/312 [02:44<1:31:56, 18.21s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/312 [03:02<1:31:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.943, 'grad_norm': 0.07861328125, 'learning_rate': 0.0002, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/312 [03:02<1:31:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 11/312 [03:20<1:31:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 12/312 [03:38<1:31:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 13/312 [03:57<1:30:41, 18.20s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/312 [04:15<1:30:23, 18.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 15/312 [04:33<1:30:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 16/312 [04:51<1:29:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 17/312 [05:09<1:29:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 18/312 [05:28<1:29:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 19/312 [05:46<1:28:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 20/312 [06:04<1:28:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7615, 'grad_norm': 0.06884765625, 'learning_rate': 0.0002, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m6%|▋         | 20/312 [06:04<1:28:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 21/312 [06:22<1:28:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 22/312 [06:40<1:27:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/312 [06:59<1:27:39, 18.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 24/312 [07:17<1:27:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 25/312 [07:35<1:27:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 26/312 [07:53<1:26:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 27/312 [08:11<1:26:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 28/312 [08:30<1:26:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 29/312 [08:48<1:25:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 30/312 [09:06<1:25:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6877, 'grad_norm': 0.08251953125, 'learning_rate': 0.0002, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m10%|▉         | 30/312 [09:06<1:25:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 31/312 [09:24<1:25:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 32/312 [09:42<1:24:55, 18.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 33/312 [10:01<1:24:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 34/312 [10:19<1:24:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 35/312 [10:37<1:24:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 36/312 [10:55<1:23:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 37/312 [11:13<1:23:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 38/312 [11:32<1:23:11, 18.22s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 39/312 [11:50<1:22:52, 18.21s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 40/312 [12:08<1:22:32, 18.21s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.618, 'grad_norm': 0.1044921875, 'learning_rate': 0.0002, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 40/312 [12:08<1:22:32, 18.21s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 41/312 [12:26<1:22:13, 18.21s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 42/312 [12:44<1:21:55, 18.20s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 43/312 [13:03<1:21:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 44/312 [13:21<1:21:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 45/312 [13:39<1:20:59, 18.20s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 46/312 [13:57<1:20:41, 18.20s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 47/312 [14:15<1:20:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 48/312 [14:34<1:20:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 49/312 [14:52<1:19:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 50/312 [15:10<1:19:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5707, 'grad_norm': 0.0634765625, 'learning_rate': 0.0002, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 50/312 [15:10<1:19:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 51/312 [15:28<1:19:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 52/312 [15:46<1:18:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 53/312 [16:05<1:18:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 74/312 [22:27<1:12:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 75/312 [22:45<1:11:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 76/312 [23:03<1:11:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 77/312 [23:21<1:11:16, 18.20s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 78/312 [23:40<1:10:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 79/312 [23:58<1:10:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 80/312 [24:16<1:10:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5151, 'grad_norm': 0.0673828125, 'learning_rate': 0.0002, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 80/312 [24:16<1:10:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 81/312 [24:34<1:10:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 82/312 [24:52<1:09:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 83/312 [25:11<1:09:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 84/312 [25:29<1:09:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 85/312 [25:47<1:08:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 86/312 [26:05<1:08:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 87/312 [26:23<1:08:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 88/312 [26:42<1:07:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 89/312 [27:00<1:07:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 90/312 [27:18<1:07:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4943, 'grad_norm': 0.080078125, 'learning_rate': 0.0002, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 90/312 [27:18<1:07:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 91/312 [27:36<1:07:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 92/312 [27:54<1:06:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 93/312 [28:13<1:06:25, 18.20s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 94/312 [28:31<1:06:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 95/312 [28:49<1:05:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 96/312 [29:07<1:05:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 97/312 [29:25<1:05:12, 18.20s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 98/312 [29:44<1:04:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 99/312 [30:02<1:04:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 100/312 [30:20<1:04:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4998, 'grad_norm': 0.076171875, 'learning_rate': 0.0002, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 100/312 [30:20<1:04:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 101/312 [30:38<1:04:04, 18.22s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 102/312 [30:56<1:03:44, 18.21s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 103/312 [31:15<1:03:25, 18.21s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 104/312 [31:33<1:03:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 105/312 [31:51<1:03:17, 18.34s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 106/312 [32:10<1:02:49, 18.30s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 107/312 [32:28<1:02:25, 18.27s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 108/312 [32:46<1:02:02, 18.25s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 109/312 [33:04<1:01:41, 18.23s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 110/312 [33:22<1:01:21, 18.22s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4843, 'grad_norm': 0.07763671875, 'learning_rate': 0.0002, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 110/312 [33:22<1:01:21, 18.22s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 111/312 [33:41<1:01:01, 18.22s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 112/312 [33:59<1:00:41, 18.21s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 113/312 [34:17<1:00:23, 18.21s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 114/312 [34:35<1:00:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 115/312 [34:53<59:45, 18.20s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 116/312 [35:12<59:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 117/312 [35:30<59:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 118/312 [35:48<58:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 119/312 [36:06<58:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 120/312 [36:24<58:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.477, 'grad_norm': 0.08984375, 'learning_rate': 0.0002, 'epoch': 1.15}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 120/312 [36:24<58:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 121/312 [36:43<57:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 122/312 [37:01<57:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 123/312 [37:19<57:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 124/312 [37:37<57:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 125/312 [37:55<56:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 126/312 [38:14<56:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 127/312 [38:32<56:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 128/312 [38:50<55:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 129/312 [39:08<55:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 130/312 [39:26<55:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4697, 'grad_norm': 0.08544921875, 'learning_rate': 0.0002, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 130/312 [39:26<55:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 131/312 [39:45<54:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 132/312 [40:03<54:35, 18.20s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 133/312 [40:21<54:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 134/312 [40:39<53:59, 18.20s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 135/312 [40:57<53:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 136/312 [41:16<53:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 137/312 [41:34<53:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 138/312 [41:52<52:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 139/312 [42:10<52:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 140/312 [42:28<52:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4633, 'grad_norm': 0.07861328125, 'learning_rate': 0.0002, 'epoch': 1.35}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 140/312 [42:28<52:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 141/312 [42:47<51:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 142/312 [43:05<51:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 143/312 [43:23<51:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 144/312 [43:41<50:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 145/312 [43:59<50:39, 18.20s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 146/312 [44:18<50:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 147/312 [44:36<50:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 148/312 [44:54<49:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 149/312 [45:12<49:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 150/312 [45:30<49:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.465, 'grad_norm': 0.08349609375, 'learning_rate': 0.0002, 'epoch': 1.44}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 150/312 [45:30<49:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 151/312 [45:49<48:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 152/312 [46:07<48:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 153/312 [46:25<48:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 154/312 [46:43<47:58, 18.22s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 155/312 [47:01<47:39, 18.21s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 156/312 [47:20<47:20, 18.21s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 157/312 [47:38<47:01, 18.21s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 158/312 [47:56<46:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 159/312 [48:14<46:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 160/312 [48:32<46:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.454, 'grad_norm': 0.08740234375, 'learning_rate': 0.0002, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 160/312 [48:32<46:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 161/312 [48:51<45:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 162/312 [49:09<45:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 163/312 [49:27<45:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 164/312 [49:45<44:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 165/312 [50:03<44:35, 18.20s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 166/312 [50:22<44:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 167/312 [50:40<43:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 168/312 [50:58<43:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 169/312 [51:16<43:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 170/312 [51:34<43:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4539, 'grad_norm': 0.07861328125, 'learning_rate': 0.0002, 'epoch': 1.63}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 170/312 [51:34<43:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 171/312 [51:53<42:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 172/312 [52:11<42:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 173/312 [52:29<42:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 174/312 [52:47<41:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 175/312 [53:05<41:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 176/312 [53:24<41:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 177/312 [53:42<40:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 178/312 [54:00<40:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 179/312 [54:18<40:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 180/312 [54:36<40:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4509, 'grad_norm': 0.08056640625, 'learning_rate': 0.0002, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 180/312 [54:36<40:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 181/312 [54:55<39:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 182/312 [55:13<39:25, 18.20s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 183/312 [55:31<39:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 184/312 [55:49<38:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 185/312 [56:07<38:31, 18.20s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 186/312 [56:26<38:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 187/312 [56:44<37:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 188/312 [57:02<37:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 189/312 [57:20<37:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 190/312 [57:38<37:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4461, 'grad_norm': 0.087890625, 'learning_rate': 0.0002, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[34m61%|██████    | 190/312 [57:38<37:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 191/312 [57:57<36:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 192/312 [58:15<36:23, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 193/312 [58:33<36:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 194/312 [58:51<35:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 195/312 [59:09<35:29, 18.20s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 196/312 [59:28<35:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 197/312 [59:46<34:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 198/312 [1:00:04<34:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 199/312 [1:00:22<34:16, 18.20s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 200/312 [1:00:40<33:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4558, 'grad_norm': 0.0869140625, 'learning_rate': 0.0002, 'epoch': 1.92}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 200/312 [1:00:40<33:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 201/312 [1:00:59<33:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 202/312 [1:01:17<33:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 203/312 [1:01:35<33:03, 18.20s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 204/312 [1:01:53<32:45, 18.20s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 205/312 [1:02:11<32:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 206/312 [1:02:30<32:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 207/312 [1:02:48<31:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 208/312 [1:03:06<31:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 209/312 [1:03:25<31:33, 18.39s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 210/312 [1:03:43<31:09, 18.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4383, 'grad_norm': 0.08251953125, 'learning_rate': 0.0002, 'epoch': 2.02}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 210/312 [1:03:43<31:09, 18.33s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 211/312 [1:04:01<30:49, 18.31s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 212/312 [1:04:19<30:27, 18.28s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 213/312 [1:04:38<30:07, 18.25s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 214/312 [1:04:56<29:47, 18.24s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 215/312 [1:05:14<29:27, 18.23s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 216/312 [1:05:32<29:08, 18.22s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 217/312 [1:05:50<28:50, 18.21s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 218/312 [1:06:09<28:31, 18.21s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 219/312 [1:06:27<28:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 220/312 [1:06:45<27:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4257, 'grad_norm': 0.08447265625, 'learning_rate': 0.0002, 'epoch': 2.12}\u001b[0m\n",
      "\u001b[34m71%|███████   | 220/312 [1:06:45<27:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 221/312 [1:07:03<27:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 222/312 [1:07:21<27:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 223/312 [1:07:40<27:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 224/312 [1:07:58<26:41, 18.20s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 225/312 [1:08:16<26:23, 18.20s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 226/312 [1:08:34<26:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 227/312 [1:08:52<25:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 228/312 [1:09:11<25:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 229/312 [1:09:29<25:10, 18.20s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 230/312 [1:09:47<24:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4192, 'grad_norm': 0.0947265625, 'learning_rate': 0.0002, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 230/312 [1:09:47<24:52, 18.20s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 231/312 [1:10:05<24:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 232/312 [1:10:23<24:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 233/312 [1:10:42<23:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 234/312 [1:11:00<23:39, 18.20s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 235/312 [1:11:18<23:21, 18.20s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 236/312 [1:11:36<23:03, 18.20s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 237/312 [1:11:54<22:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 238/312 [1:12:13<22:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 239/312 [1:12:31<22:08, 18.20s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 240/312 [1:12:49<21:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4149, 'grad_norm': 0.10205078125, 'learning_rate': 0.0002, 'epoch': 2.31}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 240/312 [1:12:49<21:50, 18.20s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 241/312 [1:13:07<21:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 242/312 [1:13:25<21:13, 18.20s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 243/312 [1:13:44<20:55, 18.20s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 244/312 [1:14:02<20:37, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 245/312 [1:14:20<20:19, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 246/312 [1:14:38<20:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 247/312 [1:14:56<19:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 248/312 [1:15:15<19:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 249/312 [1:15:33<19:06, 18.20s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 250/312 [1:15:51<18:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4248, 'grad_norm': 0.0986328125, 'learning_rate': 0.0002, 'epoch': 2.4}\u001b[0m\n",
      "\u001b[34m80%|████████  | 250/312 [1:15:51<18:48, 18.20s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 251/312 [1:16:09<18:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 252/312 [1:16:27<18:12, 18.20s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 253/312 [1:16:46<17:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 254/312 [1:17:04<17:35, 18.20s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 255/312 [1:17:22<17:17, 18.20s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 256/312 [1:17:40<16:59, 18.20s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 257/312 [1:17:58<16:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 258/312 [1:18:17<16:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 259/312 [1:18:35<16:04, 18.20s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 260/312 [1:18:53<15:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4198, 'grad_norm': 0.099609375, 'learning_rate': 0.0002, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 260/312 [1:18:53<15:46, 18.20s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 261/312 [1:19:11<15:28, 18.20s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 262/312 [1:19:29<15:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 263/312 [1:19:48<14:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 264/312 [1:20:06<14:33, 18.20s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 265/312 [1:20:24<14:15, 18.20s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 266/312 [1:20:42<13:57, 18.20s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 267/312 [1:21:00<13:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 268/312 [1:21:19<13:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 269/312 [1:21:37<13:02, 18.20s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 270/312 [1:21:55<12:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4163, 'grad_norm': 0.1005859375, 'learning_rate': 0.0002, 'epoch': 2.6}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 270/312 [1:21:55<12:44, 18.20s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 271/312 [1:22:13<12:26, 18.20s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 272/312 [1:22:31<12:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 273/312 [1:22:50<11:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 274/312 [1:23:08<11:32, 18.22s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 275/312 [1:23:26<11:13, 18.21s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 276/312 [1:23:44<10:55, 18.21s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 277/312 [1:24:02<10:37, 18.21s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 278/312 [1:24:21<10:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 279/312 [1:24:39<10:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 280/312 [1:24:57<09:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4197, 'grad_norm': 0.099609375, 'learning_rate': 0.0002, 'epoch': 2.69}\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 280/312 [1:24:57<09:42, 18.20s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 281/312 [1:25:15<09:24, 18.20s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 282/312 [1:25:33<09:05, 18.20s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 283/312 [1:25:52<08:47, 18.20s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 284/312 [1:26:10<08:29, 18.20s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 285/312 [1:26:28<08:11, 18.20s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 286/312 [1:26:46<07:53, 18.20s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 287/312 [1:27:04<07:34, 18.20s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 288/312 [1:27:23<07:16, 18.20s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 289/312 [1:27:41<06:58, 18.20s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 290/312 [1:27:59<06:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4145, 'grad_norm': 0.10498046875, 'learning_rate': 0.0002, 'epoch': 2.79}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 290/312 [1:27:59<06:40, 18.20s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 291/312 [1:28:17<06:22, 18.20s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 292/312 [1:28:35<06:03, 18.20s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 293/312 [1:28:54<05:45, 18.20s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 294/312 [1:29:12<05:27, 18.20s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 295/312 [1:29:30<05:09, 18.20s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 296/312 [1:29:48<04:51, 18.20s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 297/312 [1:30:06<04:32, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 298/312 [1:30:25<04:14, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 299/312 [1:30:43<03:56, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 300/312 [1:31:01<03:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4177, 'grad_norm': 0.1123046875, 'learning_rate': 0.0002, 'epoch': 2.88}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 300/312 [1:31:01<03:38, 18.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 301/312 [1:31:19<03:20, 18.20s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 302/312 [1:31:37<03:01, 18.20s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 303/312 [1:31:56<02:43, 18.20s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 304/312 [1:32:14<02:25, 18.20s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 305/312 [1:32:32<02:07, 18.20s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 306/312 [1:32:50<01:49, 18.20s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 307/312 [1:33:08<01:30, 18.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 308/312 [1:33:27<01:12, 18.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 309/312 [1:33:45<00:54, 18.20s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 310/312 [1:34:03<00:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4224, 'grad_norm': 0.1015625, 'learning_rate': 0.0002, 'epoch': 2.98}\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 310/312 [1:34:03<00:36, 18.20s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 311/312 [1:34:21<00:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 312/312 [1:34:39<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 5680.3818, 'train_samples_per_second': 0.22, 'train_steps_per_second': 0.055, 'train_loss': 0.49664624474751645, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 312/312 [1:34:40<00:00, 18.20s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 312/312 [1:34:40<00:00, 18.21s/it]\u001b[0m\n",
      "\u001b[34m['README.md', 'adapter_config.json', 'runs', 'checkpoint-208', 'checkpoint-312', 'tokenizer.json', 'adapter_model.safetensors', 'special_tokens_map.json', 'tokenizer.model', 'checkpoint-104', 'tokenizer_config.json']\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 14.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.32s/it]\u001b[0m\n",
      "\u001b[34m2024-05-01 06:57:16,301 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-01 06:57:16,301 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-01 06:57:16,301 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-05-01 06:57:20 Uploading - Uploading generated training model\n",
      "2024-05-01 06:58:06 Completed - Training job completed\n",
      "Training seconds: 6167\n",
      "Billable seconds: 6167\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245f304f-6898-4589-b3f5-f8a4354d72c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_data\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3DataSource\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3Uri\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://s3.console.aws.amazon.com/s3/buckets/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1858\u001b[0m, in \u001b[0;36mEstimatorBase.model_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3DataSource\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   1848\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3Uri\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         }\n\u001b[1;32m   1852\u001b[0m     }\n\u001b[1;32m   1854\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1855\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo finished training job found associated with this estimator. Please make sure \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis estimator is only used for building workflow config\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1857\u001b[0m )\n\u001b[0;32m-> 1858\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_current_job_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_uri\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/posixpath.py:76\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(a, \u001b[38;5;241m*\u001b[39mp):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    If any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    will be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    ends with a separator.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(a)\n\u001b[1;32m     78\u001b[0m     path \u001b[38;5;241m=\u001b[39m a\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"].replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9076623f-df21-4153-9932-f5c2b1619b13",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    " \n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a10489-4194-4fa9-a560-2336cbd4454e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# s3 path where the model will be uploaded\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# if you try to deploy the model to a different time add the s3 path here\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model_s3_path \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_data\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3DataSource\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3Uri\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# sagemaker config\u001b[39;00m\n\u001b[1;32m      9\u001b[0m instance_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.g5.2xlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1858\u001b[0m, in \u001b[0;36mEstimatorBase.model_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3DataSource\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   1848\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3Uri\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         }\n\u001b[1;32m   1852\u001b[0m     }\n\u001b[1;32m   1854\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1855\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo finished training job found associated with this estimator. Please make sure \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis estimator is only used for building workflow config\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1857\u001b[0m )\n\u001b[0;32m-> 1858\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_current_job_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_uri\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/posixpath.py:76\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(a, \u001b[38;5;241m*\u001b[39mp):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    If any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    will be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    ends with a separator.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(a)\n\u001b[1;32m     78\u001b[0m     path \u001b[38;5;241m=\u001b[39m a\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46232d4c-490e-4d49-b636-9a800c4a0d44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Deploy model to an endpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m      4\u001b[0m   initial_instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m   instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[1;32m      6\u001b[0m   container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mhealth_check_timeout, \u001b[38;5;66;03m# 10 minutes to give SageMaker the time to download the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_model' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8b3df11-e3ed-4a23-8b84-a921723d1239",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the endpoint again\n",
    "from sagemaker.huggingface import HuggingFacePredictor\n",
    "\n",
    "llm = HuggingFacePredictor(\"huggingface-pytorch-tgi-inference-2024-05-01-07-07-29-937\", sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adb8298c-77ce-4f19-a87a-5f79cbf4fe33",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69f5ab17bfe4027ad18dc593d6098d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'What is the rank of the film directed by danny devito?', 'role': 'user'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'SELECT rank FROM table_name_32 WHERE director = \"danny devito\"'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    " \n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    " \n",
    "# Load the test dataset from s3\n",
    "S3Downloader.download(f\"{training_input_path}/test_dataset.json\", \".\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\",split=\"train\")\n",
    "random_sample = test_dataset[345]\n",
    " \n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    " \n",
    "print(random_sample[\"messages\"][1])\n",
    "request(random_sample[\"messages\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d44f7fbb-b9da-4344-b733-cbd7b74bd988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [15:41<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    " \n",
    "def evaluate(sample):\n",
    "    predicted_answer = request(sample[\"messages\"][:2])\n",
    "    if predicted_answer[\"content\"] == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "success_rate = []\n",
    "number_of_eval_samples = 1000\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(test_dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    " \n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    " \n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74941457-e3cc-42d9-8897-e25a1fac3c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
